Question Number,Question Text,Options,Answer (Blue Tick)
244,"You have a Bigtable instance that consists of three nodes that store personally identifiable information (PII) data. You need to log all read or write operations, including any metadata or configuration reads of this database table, in your company’s Security Information and Event Management (SIEM) system. What should you do?","A. • Navigate to Cloud Monitoring in the Google Cloud console, and create a custom monitoring job for the Bigtable instance to track all changes. • Create an alert by using webhook endpoints, with the SIEM endpoint as a receiver. | B. • Navigate to the Audit Logs page in the Google Cloud console, and enable Admin Write logs for the Bigtable instance. • Create a Cloud Functions instance to export logs from Cloud Logging to your SIEM. | C. • Navigate to the Audit Logs page in the Google Cloud console, and enable Data Read, Data Write and Admin Read logs for the Bigtable instance. • Create a Pub/Sub topic as a Cloud Logging sink destination, and add your SIEM as a subscriber to the topic. | D. • Install the Ops Agent on the Bigtable instance during configuration. • Create a service account with read permissions for the Bigtable instance. • Create a custom Dataflow job with this service account to export logs to the company’s SIEM system.",C
245,"You want to set up a Google Kubernetes Engine cluster. Verifiable node identity and integrity are required for the cluster, and nodes cannot be accessed from the internet. You want to reduce the operational cost of managing your cluster, and you want to follow Google-recommended practices. What should you do?","A. Deploy a private autopilot cluster. | B. Deploy a public autopilot cluster. | C. Deploy a standard public cluster and enable shielded nodes. | D. Deploy a standard private cluster and enable shielded nodes.",A
246,"Your company wants to migrate their on-premises workloads to Google Cloud. The current on-premises workloads consist of: • A Flask web application • A backend API • A scheduled long-running background job for ETL and reporting You need to keep operational costs low. You want to follow Google-recommended practices to migrate these workloads to serverless solutions on Google Cloud. What should you do?","A. Migrate the web application to App Engine and the backend API to Cloud Run. Use Cloud Tasks to run your background job on Compute Engine. | B. Migrate the web application to App Engine and the backend API to Cloud Run. Use Cloud Tasks to run your background job on Cloud Run. | C. Run the web application on a Cloud Storage bucket and the backend API on Cloud Run. Use Cloud Tasks to run your background job on Cloud Run. | D. Run the web application on a Cloud Storage bucket and the backend API on Cloud Run. Use Cloud Tasks to run your background job on Compute Engine.",B
247,"Your company is moving its continuous integration and delivery (CI/CD) pipeline to Compute Engine instances. The pipeline will manage the entire cloud infrastructure through code. How can you ensure that the pipeline has appropriate permissions while your system is following security best practices?","A. • Attach a single service account to the compute instances. • Add minimal rights to the service account. • Allow the service account to impersonate a Cloud Identity user with elevated permissions to create, update, or delete resources. | B. • Add a step for human approval to the CI/CD pipeline before the execution of the infrastructure provisioning. • Use the human approvals IAM account for the provisioning. | C. • Attach a single service account to the compute instances. • Add all required Identity and Access Management (IAM) permissions to this service account to create, update, or delete resources. | D. • Create multiple service accounts, one for each pipeline with the appropriate minimal Identity and Access Management (IAM) permissions. • Use a secret manager service to store the key files of the service accounts. • Allow the CI/CD pipeline to request the appropriate secrets during the execution of the pipeline.",D
248,"Your application stores files on Cloud Storage by using the Standard Storage class. The application only requires access to files created in the last 30 days. You want to automatically save costs on files that are no longer accessed by the application. What should you do?","A. Create an object lifecycle on the storage bucket to change the storage class to Archive Storage for objects with an age over 30 days. | B. Create a cron job in Cloud Scheduler to call a Cloud Functions instance every day to delete files older than 30 days. | C. Create a retention policy on the storage bucket of 30 days, and lock the bucket by using a retention policy lock. | D. Enable object versioning on the storage bucket and add lifecycle rules to expire non-current versions after 30 days.",A
249,"Your manager asks you to deploy a workload to a Kubernetes cluster. You are not sure of the workload's resource requirements or how the requirements might vary depending on usage patterns, external dependencies, or other factors. You need a solution that makes cost-effective recommendations regarding CPU and memory requirements, and allows the workload to function consistently in any situation. You want to follow Google-recommended practices. What should you do?","A. Configure the Horizontal Pod Autoscaler for availability, and configure the cluster autoscaler for suggestions. | B. Configure the Horizontal Pod Autoscaler for availability, and configure the Vertical Pod Autoscaler recommendations for suggestions. | C. Configure the Vertical Pod Autoscaler recommendations for availability, and configure the Cluster autoscaler for suggestions. | D. Configure the Vertical Pod Autoscaler recommendations for availability, and configure the Horizontal Pod Autoscaler for suggestions.",B
250,"You need to migrate invoice documents stored on-premises to Cloud Storage. The documents have the following storage requirements: • Documents must be kept for five years. • Up to five revisions of the same invoice document must be stored, to allow for corrections. • Documents older than 365 days should be moved to lower cost storage tiers. You want to follow Google-recommended practices to minimize your operational and development costs. What should you do?","A. Enable retention policies on the bucket, and use Cloud Scheduler to invoke a Cloud Function to move or delete your documents based on their metadata. | B. Enable retention policies on the bucket, use lifecycle rules to change the storage classes of the objects, set the number of versions, and delete old files. | C. Enable object versioning on the bucket, and use Cloud Scheduler to invoke a Cloud Functions instance to move or delete your documents based on their metadata. | D. Enable object versioning on the bucket, use lifecycle conditions to change the storage class of the objects, set the number of versions, and delete old files.",D
251,"You installed the Google Cloud CLI on your workstation and set the proxy configuration. However, you are worried that your proxy credentials will be recorded in the gcloud CLI logs. You want to prevent your proxy credential from being logged. What should you do?","A. Configure username and password by using gcloud config set proxy/username and gcloud config set proxy/password commands. | B. Encode username and password in sha256 encoding, and save in to a text file. Use filename as a value in the gcloud config set core/custom_ca_certs_file command. | C. Provide values for CLOUDSDK_PROXY_USERNAME and CLOUDSDK_PROXY_PASSWORD in the gcloud CLI tool configuration file. | D. Set the CLOUDSDK_PROXY_USERNAME and CLOUDSDK_PROXY_PASSWORD properties by using environment variables in your command line tool.",D
252,"Your company developed an application to deploy on Google Kubernetes Engine. Certain parts of the application are not fault-tolerant and are allowed to have downtime. Other parts of the application are critical and must always be available. You need to configure a Google Kubernetes Engine cluster while optimizing for cost. What should you do?","A. Create a cluster with a single node-pool by using standard VMs. Label he fault-tolerant Deployments as spot_true. | B. Create a cluster with a single node-pool by using Spot VMs. Label the critical Deployments as spot_false. | C. Create a cluster with both a Spot VM node pool and a node pool by using standard VMs. Deploy the critical deployments on the Spot VM node pool and the fault-tolerant deployments on the node pool by using standard VMs. | D. Create a cluster with both a Spot VM node pool and a nods pool by using standard VMs. Deploy the critical deployments on the node pool by using standard VMs and the fault-tolerant deployments on the Spot VM node pool.",D
253,"You need to deploy an application in Google Cloud using serverless technology. You want to test a new version of the application with a small percentage of production traffic. What should you do?","A. Deploy the application to Cloud Run. Use gradual rollouts for traffic splitting. | B. Deploy the application to Google Kubernetes Engine. Use Anthos Service Mash for traffic splitting. | C. Deploy the application to Cloud Functions. Specify the version number in the functions name. | D. Deploy the application to App Engine. For each new version, create a new service.",A
254,"Your company's security vulnerability management policy wants a member of the security team to have visibility into vulnerabilities and other OS metadata for a specific Compute Engine instance. This Compute Engine instance hosts a critical application in your Google Cloud project. You need to implement your company's security vulnerability management policy. What should you do?","A. • Ensure that the Ops Agent is installed on the Compute Engine instance. • Create a custom metric in the Cloud Monitoring dashboard. • Provide the security team member with access to this dashboard. | B. • Ensure that the Ops Agent is installed on the Compute Engine instance. • Provide the security team member roles/osconfig.inventoryViewer permission. | C. • Ensure that the OS Config agent is installed on the Compute Engine instance. • Provide the security team member roles/osconfig.vulnerabilityReportViewer permission. | D. • Ensure that the OS Config agent is installed on the Compute Engine instance. • Create a log sink to BigQuery dataset. • Provide the security team member with access to this dataset.",C
255,"You want to enable your development team to deploy new features to an existing Cloud Run service in production. To minimize the risk associated with a new revision, you want to reduce the number of customers who might be affected by an outage without introducing any development or operational costs to your customers. You want to follow Google-recommended practices for managing revisions to a service. What should you do?","A. Ask your customers to retry access to your service with exponential backoff to mitigate any potential problems after the new revision is deployed. | B. Gradually roll out the new revision and split customer traffic between the revisions to allow rollback in case a problem occurs. | C. Send all customer traffic to the new revision, and roll back to a previous revision if you witness any problems in production. | D. Deploy your application to a second Cloud Run service, and ask your customers to use the second Cloud Run service.",B
256,"You have deployed an application on a Compute Engine instance. An external consultant needs to access the Linux-based instance. The consultant is connected to your corporate network through a VPN connection, but the consultant has no Google account. What should you do?","A. Instruct the external consultant to use the gcloud compute ssh command line tool by using Identity-Aware Proxy to access the instance. | B. Instruct the external consultant to use the gcloud compute ssh command line tool by using the public IP address of the instance to access it. | C. Instruct the external consultant to generate an SSH key pair, and request the public key from the consultant. Add the public key to the instance yourself, and have the consultant access the instance through SSH with their private key. | D. Instruct the external consultant to generate an SSH key pair, and request the private key from the consultant. Add the private key to the instance yourself, and have the consultant access the instance through SSH with their public key.",C
257,"After a recent security incident, your startup company wants better insight into what is happening in the Google Cloud environment. You need to monitor unexpected firewall changes and instance creation. Your company prefers simple solutions. What should you do?","A. Create a log sink to forward Cloud Audit Logs filtered for firewalls and compute instances to Cloud Storage. Use BigQuery to periodically analyze log events in the storage bucket. | B. Use Cloud Logging filters to create log-based metrics for firewall and instance actions. Monitor the changes and set up reasonable alerts. | C. Install Kibana on a compute instance. Create a log sink to forward Cloud Audit Logs filtered for firewalls and compute instances to Pub/Sub. Target the Pub/Sub topic to push messages to the Kibana instance. Analyze the logs on Kibana in real time. | D. Turn on Google Cloud firewall rules logging, and set up alerts for any insert, update, or delete events.",B
258,"You are configuring service accounts for an application that spans multiple projects. Virtual machines (VMs) running in the web-applications project need access to BigQuery datasets in the crm-databases project. You want to follow Google-recommended practices to grant access to the service account in the web-applications project. What should you do?","A. Grant ""project owner"" for web-applications appropriate roles to crm-databases. | B. Grant ""project owner"" role to crm-databases and the web-applications project. | C. Grant ""project owner"" role to crm-databases and roles/bigquery.dataViewer role to web-applications. | D. Grant roles/bigquery.dataViewer role to crm-databases and appropriate roles to web-applications.",D
259,"Your Dataproc cluster runs in a single Virtual Private Cloud (VPC) network in a single subnetwork with range 172.16.20.128/25. There are no private IP addresses available in the subnetwork. You want to add new VMs to communicate with your cluster using the minimum number of steps. What should you do?","A. Modify the existing subnet range to 172.16.20.0/24. | B. Create a new Secondary IP Range in the VPC and configure the VMs to use that range. | C. Create a new VPC network for the VMs. Enable VPC Peering between the VMs'VPC network and the Dataproc cluster VPC network. | D. Create a new VPC network for the VMs with a subnet of 172.32.0.0/16. Enable VPC network Peering between the Dataproc VPC network and the VMs VPC network. Configure a custom Route exchange.",A
260,"You are building a backend service for an ecommerce platform that will persist transaction data from mobile and web clients. After the platform is launched, you expect a large volume of global transactions. Your business team wants to run SQL queries to analyze the data. You need to build a highly available and scalable data store for the platform. What should you do?","A. Create a multi-region Cloud Spanner instance with an optimized schema. | B. Create a multi-region Firestore database with aggregation query enabled. | C. Create a multi-region Cloud SQL for PostgreSQL database with optimized indexes. | D. Create a multi-region BigQuery dataset with optimized tables.",A
261,"You are in charge of provisioning access for all Google Cloud users in your organization. Your company recently acquired a startup company that has their own Google Cloud organization. You need to ensure that your Site Reliability Engineers (SREs) have the same project permissions in the startup company's organization as in your own organization. What should you do?","A. In the Google Cloud console for your organization, select Create role from selection, and choose destination as the startup company's organization. | B. In the Google Cloud console for the startup company, select Create role from selection and choose source as the startup company's Google Cloud organization. | C. Use the gcloud iam roles copy command, and provide the Organization ID of the startup company's Google Cloud Organization as the destination. | D. Use the gcloud iam roles copy command, and provide the project IDs of all projects in the startup company's organization as the destination.",C
262,"You need to extract text from audio files by using the Speech-to-Text API. The audio files are pushed to a Cloud Storage bucket. You need to implement a fully managed, serverless compute solution that requires authentication and aligns with Google-recommended practices. You want to automate the call to the API by submitting each file to the API as the audio file arrives in the bucket. What should you do?","A. Create an App Engine standard environment triggered by Cloud Storage bucket events to submit the file URI to the Google Speech-to TextAPI. | B. Run a Kubernetes job to scan the bucket regularly for incoming files, and call the Speech-to-Text API for each unprocessed file. | C. Run a Python script by using a Linux cron job in Compute Engine to scan the bucket regularly for incoming files, and call the Speech-to-Text API for each unprocessed file. | D. Create a Cloud Function triggered by Cloud Storage bucket events to submit the file URI to the Google Speech-to-Text API.",D
263,"Your customer wants you to create a secure website with autoscaling based on the compute instance CPU load. You want to enhance performance by storing static content in Cloud Storage. Which resources are needed to distribute the user traffic?","A. An external HTTP(S) load balancer with a managed SSL certificate to distribute the load and a URL map to target the requests for the static content to the Cloud Storage backend. | B. An external network load balancer pointing to the backend instances to distribute the load evenly. The web servers will forward the request to the Cloud Storage as needed. | C. An internal HTTP(S) load balancer together with Identity-Aware Proxy to allow only HTTPS traffic. | D. An external HTTP(S) load balancer to distribute the load and a URL map to target the requests for the static content to the Cloud Storage backend. Install the HTTPS certificates on the instance.",A
264,"The core business of your company is to rent out construction equipment at large scale. All the equipment that is being rented out has been equipped with multiple sensors that send event information every few seconds. These signals can vary from engine status, distance traveled, fuel level, and more. Customers are billed based on the consumption monitored by these sensors. You expect high throughput – up to thousands of events per hour per device – and need to retrieve consistent data based on the time of the event. Storing and retrieving individual signals should be atomic. What should you do?","A. Create files in Cloud Storage as data comes in. | B. Create a file in Filestore per device, and append new data to that file. | C. Ingest the data into Cloud SQL. Use multiple read replicas to match the throughput. | D. Ingest the data into Bigtable. Create a row key based on the event timestamp.",D
265,"You just installed the Google Cloud CLI on your new corporate laptop. You need to list the existing instances of your company on Google Cloud. What must you do before you run the gcloud compute instances list command? (Choose two.)","A. Run gcloud auth login, enter your login credentials in the dialog window, and paste the received login token to gcloud CLI. | B. Create a Google Cloud service account, and download the service account key. Place the key file in a folder on your machine where gcloud CLI can find it. | C. Download your Cloud Identity user account key. Place the key file in a folder on your machine where gcloud CLI can find it. | D. Run gcloud config set compute/zone $my_zone to set the default zone for gcloud CLI. | E. Run gcloud config set project $my_project to set the default project for gcloud CLI.","A"
266,"You are planning to migrate your on-premises data to Google Cloud. The data includes: • 200 TB of video files in SAN storage • Data warehouse data stored on Amazon Redshift • 20 GB of PNG files stored on an S3 bucket. You need to load the video files into a Cloud Storage bucket, transfer the data warehouse data into BigQuery, and load the PNG files into a second Cloud Storage bucket. You want to follow Google-recommended practices and avoid writing any code for the migration. What should you do?","A. Use gcloud storage for the video files, Dataflow for the data warehouse data, and Storage Transfer Service for the PNG files. | B. Use Transfer Appliance for the videos, BigQuery Data Transfer Service for the data warehouse data, and Storage Transfer Service for the PNG files. | C. Use Storage Transfer Service for the video files, BigQuery Data Transfer Service for the data warehouse data, and Storage Transfer Service for the PNG files. | D. Use Cloud Data Fusion for the video files, Dataflow for the data warehouse data, and Storage Transfer Service for the PNG files.",B
268,"Your team is building a website that handles votes from a large user population. The incoming votes will arrive at various rates. You want to optimize the storage and processing of the votes. What should you do?","A. Save the incoming votes to Firestore. Use Cloud Scheduler to trigger a Cloud Functions instance to periodically process the votes. | B. Use a dedicated instance to process the incoming votes. Send the votes directly to this instance. | C. Save the incoming votes to a JSON file on Cloud Storage. Process the votes in a batch at the end of the day. | D. Save the incoming votes to Pub/Sub. Use the Pub/Sub topic to trigger a Cloud Functions instance to process the votes.",D
269,"You are deploying an application on Google Cloud that requires a relational database for storage. To satisfy your company’s security policies, your application must connect to your database through an encrypted and authenticated connection that requires minimal management and integrates with Identity and Access Management (IAM). What should you do?","A. Deploy a Cloud SQL database with the SSL mode set to encrypted only, configure SSL/TLS client certificates, and configure a database user and password. | B. Deploy a Cloud SQL database with the SSL mode set to encrypted only, configure SSL/TLS client certificates, and configure IAM database authentication. | C. Deploy a Cloud SQL database and configure IAM database authentication. Access the database through the Cloud SQL Auth Proxy. | D. Deploy a Cloud SQL database and configure a database user and password. Access the database through the Cloud SQL Auth Proxy.",C
270,"You have two Google Cloud projects: project-a with VPC vpc-a (10.0.0.0/16) and project-b with VPC vpc-b (10.8.0.0/16). Your frontend application resides in vpc-a and the backend API services are deployed in vpc-b. You need to efficiently and cost-effectively enable communication between these Google Cloud projects. You also want to follow Google-recommended practices. What should you do?","A. Create an OpenVPN connection between vpc-a and vpc-b. | B. Create VPC Network Peering between vpc-a and vpc-b. | C. Configure a Cloud Router in vpc-a and another Cloud Router in vpc-b. | D. Configure a Cloud Interconnect connection between vpc-a and vpc-b.",B
271,"Your company is running a critical workload on a single Compute Engine VM instance. Your company's disaster recovery policies require you to back up the entire instance’s disk data every day. The backups must be retained for 7 days. You must configure a backup solution that complies with your company’s security policies and requires minimal setup and configuration. What should you do?","A. Configure the instance to use persistent disk asynchronous replication. | B. Configure daily scheduled persistent disk snapshots with a retention period of 7 days. | C. Configure Cloud Scheduler to trigger a Cloud Function each day that creates a new machine image and deletes machine images that are older than 7 days. | D. Configure a bash script using gsutil to run daily through a cron job. Copy the disk’s files to a Cloud Storage bucket with archive storage class and an object lifecycle rule to delete the objects after 7 days.",B
272,"Your company requires that Google Cloud products are created with a specific configuration to comply with your company’s security policies. You need to implement a mechanism that will allow software engineers at your company to deploy and update Google Cloud products in a preconfigured and approved manner. What should you do?","A. Create Java packages that utilize the Google Cloud Client Libraries for Java to configure Google Cloud products. Store and share the packages in a source code repository. | B. Create bash scripts that utilize the Google Cloud CLI to configure Google Cloud products. Store and share the bash scripts in a source code repository. | C. Use the Google Cloud APIs by using curl to configure Google Cloud products. Store and share the curl commands in a source code repository. | D. Create Terraform modules that utilize the Google Cloud Terraform Provider to configure Google Cloud products. Store and share the modules in a source code repository.",D
273,"You are a Google Cloud organization administrator. You need to configure organization policies and log sinks on Google Cloud projects that cannot be removed by project users to comply with your company's security policies. The security policies are different for each company department. Each company department has a user with the Project Owner role assigned to their projects. What should you do?","A. Use a standard naming convention for projects that includes the department name. Configure organization policies on the organization and log sinks on the projects. | B. Use a standard naming convention for projects that includes the department name. Configure both organization policies and log sinks on the projects. | C. Organize projects under folders for each department. Configure both organization policies and log sinks on the folders. | D. Organize projects under folders for each department. Configure organization policies on the organization and log sinks on the folders.",C
274,"You are deploying a web application using Compute Engine. You created a managed instance group (MIG) to host the application. You want to follow Google-recommended practices to implement a secure and highly available solution. What should you do?","A. Use SSL proxy load balancing for the MIG and an A record in your DNS private zone with the load balancer's IP address. | B. Use SSL proxy load balancing for the MIG and a CNAME record in your DNS public zone with the load balancer’s IP address. | C. Use HTTP(S) load balancing for the MIG and a CNAME record in your DNS private zone with the load balancer’s IP address. | D. Use HTTP(S) load balancing for the MIG and an A record in your DNS public zone with the load balancer’s IP address.",D
275,"You have several hundred microservice applications running in a Google Kubernetes Engine (GKE) cluster. Each microservice is a deployment with resource limits configured for each container in the deployment. You've observed that the resource limits for memory and CPU are not appropriately set for many of the microservices. You want to ensure that each microservice has right sized limits for memory and CPU. What should you do?","A. Configure a Vertical Pod Autoscaler for each microservice. | B. Modify the cluster's node pool machine type and choose a machine type with more memory and CPU. | C. Configure a Horizontal Pod Autoscaler for each microservice. | D. Configure GKE cluster autoscaling.",A
276,"Your company uses BigQuery to store and analyze data. Upon submitting your query in BigQuery, the query fails with a quotaExceeded error. You need to diagnose the issue causing the error. What should you do? (Choose two.)","A. Use BigQuery BI Engine to analyze the issue. | B. Use the INFORMATION_SCHEMA views to analyze the underlying issue. | C. Configure Cloud Trace to analyze the issue. | D. Search errors in Cloud Audit Logs to analyze the issue. | E. View errors in Cloud Monitoring to analyze the issue.","B, D"
277,"Your team has developed a stateless application which requires it to be run directly on virtual machines. The application is expected to receive a fluctuating amount of traffic and needs to scale automatically. You need to deploy the application. What should you do?","A. Deploy the application on a managed instance group and configure autoscaling. | B. Deploy the application on a Kubernetes Engine cluster and configure node pool autoscaling. | C. Deploy the application on Cloud Functions and configure the maximum number instances. | D. Deploy the application on Cloud Run and configure autoscaling.",A
278,"Your web application is hosted on Cloud Run and needs to query a Cloud SQL database. Every morning during a traffic spike, you notice API quota errors in Cloud SQL logs. The project has already reached the maximum API quota. You want to make a configuration change to mitigate the issue. What should you do?","A. Modify the minimum number of Cloud Run instances. | B. Use traffic splitting. | C. Modify the maximum number of Cloud Run instances. | D. Set a minimum concurrent requests environment variable for the application.",A
279,"You need to deploy a single stateless web application with a web interface and multiple endpoints. For security reasons, the web application must be reachable from an internal IP address from your company's private VPC and on-premises network. You also need to update the web application multiple times per day with minimal effort and want to manage a minimal amount of cloud infrastructure. What should you do?","A. Deploy the web application on Google Kubernetes Engine standard edition with an internal ingress. | B. Deploy the web application on Cloud Run with Private Google Access configured. | C. Deploy the web application on Cloud Run with Private Service Connect configured. | D. Deploy the web application to GKE Autopilot with Private Google Access configured.",B
280,"You use Cloud Logging to capture application logs. You now need to use SQL to analyze the application logs in Cloud Logging, and you want to follow Google-recommended practices. What should you do?","A. Develop SQL queries by using Gemini for Google Cloud. | B. Enable Log Analytics for the log bucket and create a linked dataset in BigQuery. | C. Create a schema for the storage bucket and run SQL queries for the data in the bucket. | D. Export logs to a storage bucket and create an external view in BigQuery.",B
281,"You need to deploy a third-party software application onto a single Compute Engine VM instance. The application requires the highest speed read and write disk access for the internal database. You need to ensure the instance will recover on failure. What should you do?","A. Create an instance template. Set the disk type to be an SSD Persistent Disk. Launch the instance template as part of a stateful managed instance group. | B. Create an instance template. Set the disk type to be an SSD Persistent Disk. Launch the instance template as part of a stateless managed instance group. | C. Create an instance template. Set the disk type to be Hyperdisk Extreme. Launch the instance template as part of a stateful managed instance group. | D. Create an instance template. Set the disk type to be Hyperdisk Extreme. Launch the instance template as part of a stateless managed instance group.",C
282,"You have a VM instance running in a VPC with single-stack subnets. You need to ensure that the VM instance has a fixed IP address so that other services hosted in the same VPC can communicate with the VM. You want to follow Google-recommended practices while minimizing cost. What should you do?","A. Promote the existing IP address of the VM to become a static external IP address. | B. Promote the existing IP address of the VM to become a static internal IP address. | C. Reserve a new static external IPv6 address and assign the new IP address to the VM. | D. Reserve a new static external IP address and assign the new IP address to the VM.",B
