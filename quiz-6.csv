Question Number,Question Text,Options,Answer
283,"Your preview application, deployed on a single-zone Google Kubernetes Engine (GKE) cluster in us-central1, has gained popularity. You are now ready to make the application generally available. You need to deploy the application to production while ensuring high availability and resilience. You also want to follow Google-recommended practices. What should you do?","A. Use the gcloud container clusters create command with the options --enable-multi-networking and --enable-autoscaling to create an autoscaling zonal cluster and deploy the application to it. | B. Use the gcloud container clusters create-auto command to create an autopilot cluster and deploy the application to it. | C. Use the gcloud container clusters update command with the option --region us-central1 to update the cluster and deploy the application to it. | D. Use the gcloud container clusters update command with the option --node-locations us-central1-a,us-central1-b to update the cluster and deploy the application to the nodes.",B
284,"You are developing an application that will be deployed on Google Cloud. The application will use a service account to retrieve data from BigQuery. Before you deploy your application, you want to test the permissions of this service account from your local machine to ensure there will be no authentication issues. You want to ensure that you use the most secure method while following Google-recommended practices. What should you do?","A. Generate a service account key, and configure the gcloud CLI to use this key. Issue a relevant BigQuery request through the gdoud CLI to test the access. | B. Grant the service account the BigQuery Administrator IAM role to ensure the service account has all required access. | C. Configure the gcloud CLI to use service account impersonation. Issue a relevant BigQuery request through the gcloud CLI to test the access. | D. Configure the gcloud CLI with Application Default Credentials using your user account. Issue a relevant BigQuery request through the gcloud CLI to test the access.",C
285,"Your organization is migrating to Google Cloud. You want only users with company-issued Google accounts to access your Google Cloud environment. You must ensure that users of the same department can only access resources within their own department. You want to minimize operational costs while following Google-recommended practices. What should you do?","A. Assign users to the relevant Google Groups, and provide access to cloud resources through Identity and Access Management (IAM) roles. Periodically identify and remove non-company issued Google accounts. | B. Assign users to the relevant Google Groups, and provide access to cloud resources through Identity and Access Management (IAM) roles. Use organization policies to block non-company issued emails. | C. Create a folder for each department in Resource Manager. Grant the users of each department the Folder Admin role on the folder of their department. | D. Create a folder for each department in Resource Manager. Grant all company users the Folder Admin role on the organization level.",B
286,"You are deploying an application to Cloud Run. Your application requires the use of an API that runs on Google Kubernetes Engine (GKE). You need to ensure that your Cloud Run service can privately reach the API on GKE, and you want to follow Google-recommended practices. What should you do?","A. Deploy an ingress resource on the GKE cluster to expose the API to the internet. Use Cloud Armor to filter for IP addresses that can connect to the API. On the Cloud Run service, configure the application to fetch its public IP address and update the Cloud Armor policy on startup to allow this IP address to call the API on ports 80 and 443. | B. Create an ingress firewall rule on the VPC to allow connections from 0.0.0.0/0 on ports 80 and 443. | C. Create an egress firewall rule on the VPC to allow connections to 0.0.0.0/ on ports 80 and 443. | D. Deploy an internal Application Load Balancer to expose the API on GKE to the VPC. Configure Cloud DNS with the IP address of the internal Application Load Balancer. Deploy a Serverless VPC Access connector to allow the Cloud Run service to call the API through the FQDN on Cloud DNS.",D
287,"Your company uses a multi-cloud strategy that includes Google Cloud. You want to centralize application logs in a third-party software-as-a-service (SaaS) tool from all environments. You need to integrate logs originating from Cloud Logging, and you want to ensure the export occurs with the least amount of delay possible. What should you do?","A. Create a Cloud Logging sink and configure BigQuery as the destination. Configure the SaaS tool to query BigQuery to retrieve the logs. | B. Create a Cloud Logging sink and configure Pub/Sub as the destination. Configure the SaaS tool to subscribe to the Pub/Sub topic to retrieve the logs. | C. Create a Cloud Logging sink and configure Cloud Storage as the destination. Configure the SaaS tool to read the Cloud Storage bucket to retrieve the logs. | D. Use a Cloud Scheduler cron job to trigger a Cloud Function that queries Cloud Logging and sends the logs to the SaaS tool.",B
288,"You are planning to migrate a database and a backend application to a Standard Google Kubernetes Engine (GKE) cluster. You need to prevent data loss and make sure there are enough nodes available for your backend application based on the demands of your workloads. You want to follow Google-recommended practices and minimize the amount of manual work required. What should you do?","A. Run your database as a StatefulSet. Configure cluster autoscaling to handle changes in the demands of your workloads. | B. Run your database as a single Pod. Run the resize command when you notice changes in the demands of your workloads. | C. Run your database as a DaemonSet. Run the resize command when you notice changes in the demands of your workloads. | D. Run your database as a Deployment. Configure cluster autoscaling to handle changes in the demands of your workloads.",A
289,"You are the Organization Administrator for your company's Google Cloud resources. Your company has strict compliance rules that require you to be notified about any modifications to files and documents hosted on Cloud Storage. In a recent incident, one of your team members was able to modify files and you did not receive any notifications, causing other production jobs to fail. You must ensure that you receive notifications for all changes to files and documents in Cloud Storage while minimizing management overhead. What should you do?","A. View Cloud Audit logs for all Cloud Storage files in Logs Explorer. Filter by Admin Activity logs. | B. Enable Cloud Storage object versioning on your bucket. Configure Pub/Sub notifications for your Cloud Storage buckets. | C. Enable versioning on the Cloud Storage bucket. Set up a custom script that scans versions of Cloud Storage objects being modified and alert the admin by using the script. | D. Configure Object change notifications on the Cloud Storage buckets. Send the events to Pub/Sub.",B
290,"Your company would like to store invoices and other financial documents in Google Cloud. You need to identify a Google-managed solution to store this information for your company. You must ensure that the documents are kept for a duration of three years. Your company’s analysts need frequent access to invoices from the past six months. After six months, invoices should be archived for audit purposes only. You want to minimize costs and follow Google-recommended practices. What should you do?","A. Use Cloud Storage with Object Lifecycle Management to change the object storage class to Coldline after six months. | B. Use Cloud Storage with Object Lifecycle Management to change the object storage class to Standard after six months. | C. Store your documents on Filestore, and move the documents to Cloud Storage with object storage class set to Coldline after six months. | D. Store your documents on Filestore, and move the documents to Cloud Storage with object storage class set to Standard after six months.",A
291,"You are planning to migrate your containerized workloads to Google Kubernetes Engine (GKE). You need to determine which GKE option to use. Your solution must have high availability, minimal downtime, and the ability to promptly apply security updates to your nodes. You also want to pay only for the compute resources that your workloads use without managing nodes. You want to follow Google-recommended practices and minimize operational costs. What should you do?","A. Configure a Standard regional GKE duster. | B. Configure a Standard zonal GKE duster. | C. Configure a Standard multi-zonal GKE cluster. | D. Configure an Autopilot GKE duster.",D
292,"Your company stores data from multiple sources that have different data storage requirements. These data include: 1. Customer data that is structured and read with complex queries 2. Historical log data that is large in volume and accessed infrequently 3. Real-time sensor data with high-velocity writes, which needs to be available for analysis but can tolerate some data loss. You need to design the most cost-effective storage solution that fulfills all data storage requirements. What should you do?","A. Use Firestore for customer data, Cloud Storage (Nearline) for historical logs, and Bigtable for sensor data. | B. Use Cloud SQL for customer data. Cloud Storage (Coldline) for historical logs, and BigQuery for sensor data. | C. Use Cloud SQL for customer data. Cloud Storage (Archive) for historical logs, and Bigtable for sensor data. | D. Use Spanner for all data.",C
293,"You work for a financial services company that operates as a stock market broker. Your company is planning to migrate to Google Cloud. You need to plan the network design in Google Cloud. Your design must: • Minimize the latency between all production systems. • Minimize costs related to your development environment. What should you do?","A. Create a VPC in the Standard Tier and one in the Premium Tier. Deploy production workloads in the Standard Tier and development workloads in the Premium Tier. | B. Create a VPC in the Standard Tier and one in the Premium Tier. Deploy development workloads in the Standard Tier and production workloads in the Premium Tier. | C. Create a VPC in the Premium Tier, and deploy both production and development workloads on this VPC. | D. Create a VPC in the Standard Tier, and deploy both production and development workloads on this VPC.",B
294,"You have developed an application that consists of multiple microservices, with each microservice packaged in its own Docker container image. You want to deploy the entire application on Google Kubernetes Engine so that each microservice can be scaled individually. What should you do?","A. Create and deploy a Custom Resource Definition per microservice. | B. Create and deploy a Docker Compose File. | C. Create and deploy a Job per microservice. | D. Create and deploy a Deployment per microservice.",D
295,"Your company was recently impacted by a service disruption that caused multiple Dataflow jobs to get stuck, resulting in significant downtime in downstream applications and revenue loss. You were able to resolve the issue by identifying and fixing an error you found in the code. You need to design a solution with minimal management effort to identify when jobs are stuck in the future to ensure that this issue does not occur again. What should you do?","A. Update the Dataflow job configurations to send messages to a Pub/Sub topic when there are delays. Configure a backup Dataflow job to process jobs that are delayed. Use Cloud Tasks to trigger an alert when messages are pushed to the Pub/Sub topic. | B. Set up Cloud Monitoring alerts on the data freshness metric for the Dataflow jobs to receive a notification when a certain threshold is reached. | C. Set up Error Reporting to identify stack traces that indicate slowdowns in Dataflow jobs. Set up alerts based on these log entries. | D. Use the Personalized Service Health dashboard to identify issues with Dataflow jobs across regions.",B
296,"Your company requires all developers to have the same permissions, regardless of the Google Cloud project they are working on. Your company’s security policy also restricts developer permissions to Compute Engine, Cloud Functions, and Cloud SQL. You want to implement the security policy with minimal effort. What should you do?","A. Create a custom role with Compute Engine, Cloud Functions, and Cloud SQL permissions in one project within the Google Cloud organization. Copy the role across all projects created within the organization with the gcloud iam roles copy command. Assign the role to developers in those projects. | B. Add all developers to a Google group in Google Groups for Workspace. Assign the predefined role of Compute Admin to the Google group at the Google Cloud organization level. | C. Add all developers to a Google group in Cloud Identity. Assign predefined roles for Compute Engine, Cloud Functions, and Cloud SQL permissions to the Google group for each project in the Google Cloud organization. | D. Add all developers to a Google group in Cloud Identity. Create a custom role with Compute Engine, Cloud Functions, and Cloud SQL permissions at the Google Cloud organization level. Assign the custom role to the Google group.",D
297,"You have an application running inside a Compute Engine instance. You want to provide the application with secure access to a BigQuery dataset. You must ensure that credentials are only valid for a short period of time, and your application will only have access to the intended BigQuery dataset. You want to follow Google-recommended practices and minimize your operational costs. What should you do?","A. Attach a new service account to the instance every hour, and grant the service account the BigQuery Data Viewer IAM role on the project. | B. Attach a custom service account to the instance, and grant the service account the BigQuery Data Viewer IAM role on the dataset. | C. Attach a new service account to the instance every hour, and grant the service account the BigQuery Data Viewer IAM role on the dataset. | D. Attach a custom service account to the instance, and grant the service account the BigQuery Data Viewer IAM role on the project.",C
298,"Your auditor wants to view your organization's use of data in Google Cloud. The auditor is most interested in auditing who accessed data in Cloud Storage buckets. You need to help the auditor access the data they need. What should you do?","A. Turn on Data Access Logs for the buckets they want to audit, and then build a query in the log viewer that filters on Cloud Storage. | B. Assign the appropriate permissions, and then create a Data Studio report on Admin Activity Audit Logs. | C. Assign the appropriate permissions, and then use Cloud Monitoring to review metrics. | D. Use the export logs API to provide the Admin Activity Audit Logs in the format they want.",A
299,"You need to configure IAM access audit logging in BigQuery for external auditors. You want to follow Google-recommended practices. What should you do?","A. Add the auditors group to the 'logging.viewer' and 'bigQuery.dataViewer' predefined IAM roles. | B. Add the auditors group to two new custom IAM roles. | C. Add the auditor user accounts to the 'logging.viewer' and 'bigQuery.dataViewer' predefined IAM roles. | D. Add the auditor user accounts to two new custom IAM roles.",A
300,"Your company is seeking a scalable solution to retain and explore application logs hosted on Compute Engine. You must be able to analyze your logs with SQL queries, and you want to be able to create charts to identify patterns and trends in your logs over time. You want to follow Google-recommended practices and minimize your operational costs. What should you do?","A. Use a custom script to push your application logs to BigQuery for exploration. | B. Ingest your application logs to Cloud Logging by using Ops Agent, and explore your logs in Logs Explorer. | C. Ingest your application logs to Cloud Logging by using Ops Agent, and explore your logs with Log Analytics. | D. Use a custom script to push your application logs to Cloud SQL for exploration.",C
301,"You are deploying an application to Google Kubernetes Engine (GKE). The application needs to make API calls to a private Cloud Storage bucket. You need to configure your application Pods to authenticate to the Cloud Storage API, but your organization policy prevents the usage of service account keys. You want to follow Google-recommended practices. What should you do?","A. Create the GKE cluster with Workload Identity Federation. Configure the default node service account to access the bucket. Deploy the application into the cluster so the application can use the node service account permissions. | B. Create the GKE cluster with Workload Identity Federation. Create a Google service account and a Kubernetes ServiceAccount, and configure both service accounts to use Workload Identity Federation. Attach the Kubernetes ServiceAccount to the application Pods and configure the Google service account to access the bucket with Identity and Access Management (IAM). | C. Create the GKE cluster and deploy the application. Request a security exception to create a Google service account key. Set the constraints/iam.serviceAccountKeyExpiryHours organization policy to 24 hours. | D. Create the GKE cluster and deploy the application. Request a security exception to create a Google service account key. Set the constraints/iam.serviceAccountKeyExpiryHours organization policy to 8 hours.",B
302,"You have just created a new project which will be used to deploy a globally distributed application. You will use Cloud Spanner for data storage. You want to create a Cloud Spanner instance. You want to perform the first step in preparation of creating the instance. What should you do?","A. Enable the Cloud Spanner API. | B. Configure your Cloud Spanner instance to be multi-regional. | C. Create a new VPC network with subnetworks in all desired regions. | D. Grant yourself the IAM role of Cloud Spanner Admin.",A
303,"You are managing the security configuration of your company’s Google Cloud organization. The Operations team needs specific permissions on both a Google Kubernetes Engine (GKE) cluster and a Cloud SQL instance. Two predefined Identity and Access Management (IAM) roles exist that contain a subset of the permissions needed by the team. You need to configure the necessary IAM permissions for this team while following Google-recommended practices. What should you do?","A. Create a custom IAM role that combines the permissions from the two relevant predefined roles. | B. Grant the team the two predefined IAM roles. | C. Create a custom IAM role that includes only the required permissions from the predefined roles. | D. Grant the team the IAM roles of Kubernetes Engine Admin and Cloud SQL Admin.",B
304,"Your management has asked an external auditor to review all the resources in a specific project. The security team has enabled the Organization Policy called Domain Restricted Sharing on the organization node by specifying only your Cloud Identity domain. You want the auditor to only be able to view, but not modify, the resources in that project. What should you do?","A. Ask the auditor for their Google account, and give them the Viewer role on the project. | B. Ask the auditor for their Google account, and give them the Security Reviewer role on the project. | C. Create a temporary account for the auditor in Cloud Identity, and give that account the Viewer role on the project. | D. Create a temporary account for the auditor in Cloud Identity, and give that account the Security Reviewer role on the project.",C
305,"You are planning to deploy an application to Google Cloud. Your application processes asynchronous events from Google services and must be accessible from the public Internet. You need to identify how to deploy your application. You want to follow a standardized process while minimizing development costs. You also want to have no costs when your workloads are not in use. What should you do?","A. Deploy your code to GKE. Use Pub/Sub for event delivery. | B. Deploy your code to Compute Engine. Use Pub/Sub for event delivery. | C. Deploy your code to GKE. Use Eventarc for event delivery. | D. Deploy your code to Cloud Run. Use Eventarc for event delivery.",D
306,"You need to host an application on a Compute Engine instance in a project shared with other teams. You want to prevent the other teams from accidentally causing downtime on that application. Which feature should you use?","A. Use a Shielded VM. | B. Use a Preemptible VM. | C. Use a sole-tenant node. | D. Enable deletion protection on the instance.",D
307,"You are working with a user to set up an application in a new VPC behind a firewall. The user is concerned about data egress. You want to configure the fewest open egress ports. What should you do?","A. Set up a low-priority (65534) rule that blocks all egress and a high-priority rule (1000) that allows only the appropriate ports. | B. Set up a high-priority (1000) rule that pairs both ingress and egress ports. | C. Set up a high-priority (1000) rule that blocks all egress and a low-priority (65534) rule that allows only the appropriate ports. | D. Set up a high-priority (1000) rule to allow the appropriate ports.",A
308,"Your management has asked an external auditor to review all the resources in a specific project. The security team has enabled the Organization Policy called Domain Restricted Sharing on the organization node by specifying only your Cloud Identity domain. You want the auditor to only be able to view, but not modify, the resources in that project. What should you do?","A. Ask the auditor for their Google account, and give them the Viewer role on the project. | B. Ask the auditor for their Google account, and give them the Security Reviewer role on the project. | C. Create a temporary account for the auditor in Cloud Identity, and give that account the Viewer role on the project. | D. Create a temporary account for the auditor in Cloud Identity, and give that account the Security Reviewer role on the project.",C
309,"You need to create a Compute Engine instance in a new project that doesn't exist yet. What should you do?","A. Using the Cloud SDK, create a new project, enable the Compute Engine API in that project, and then create the instance specifying your new project. | B. Enable the Compute Engine API in the Cloud Console, use the Cloud SDK to create the instance, and then use the --project flag to specify a new project. | C. Using the Cloud SDK, create the new instance, and use the --project flag to specify the new project. Answer yes when prompted by Cloud SDK to enable the Compute Engine API. | D. Enable the Compute Engine API in the Cloud Console. Go to the Compute Engine section of the Console to create a new instance, and look for the Create In A New Project option in the creation form.",A
310,"The DevOps group in your organization needs full control of Compute Engine resources in your development project. However, they should not have permission to create or update any other resources in the project. You want to follow Google’s recommendations for setting permissions for the DevOps group. What should you do?","A. Grant the basic role roles/viewer and the predefined role roles/compute.admin to the DevOps group. | B. Create an IAM policy and grant all compute.instanceAdmin.* permissions to the policy. Attach the policy to the DevOps group. | C. Create a custom role at the folder level and grant all compute.instanceAdmin.* permissions to the role. Grant the custom role to the DevOps group. | D. Grant the basic role roles/editor to the DevOps group.",A
311,"Your company uses BigQuery for data warehousing. Over time, many different business units in your company have created 1000+ datasets across hundreds of projects. Your CIO wants you to examine all datasets to find tables that contain an employee_ssn column. You want to minimize effort in performing this task. What should you do?","A. Go to Data Catalog and search for employee_ssn in the search box. | B. Write a shell script that uses the bq command line tool to loop through all the projects in your organization. | C. Write a script that loops through all the projects in your organization and runs a query on INFORMATION_SCHEMA.COLUMNS view to find the employee_ssn column. | D. Write a Cloud Dataflow job that loops through all the projects in your organization and runs a query on INFORMATION_SCHEMA.COLUMNS view to find employee_ssn column.",D
312,"You create a Deployment with 2 replicas in a Google Kubernetes Engine cluster that has a single preemptible node pool. After a few minutes, you use kubectl to examine the status of your Pod and observe that one of them is still in Pending status. What is the most likely cause?","A. The pending Pod's resource requests are too large to fit on a single node of the cluster. | B. Too many Pods are already running in the cluster, and there are not enough resources left to schedule the pending Pod. | C. The node pool is configured with a service account that does not have permission to pull the container image used by the pending Pod. | D. The pending Pod was originally scheduled on a node that has been preempted between the creation of the Deployment and your verification of the Pods' status.",B
313,"You need to manage multiple Google Cloud projects in the fewest steps possible. You want to configure the Google Cloud SDK command line interface (CLI) so that you can easily manage multiple projects. What should you do?","A. 1. Create a configuration for each project you need to manage. 2. Activate the appropriate configuration when you work with each of your assigned Google Cloud projects. | B. 1. Create a configuration for each project you need to manage. 2. Use gcloud init to update the configuration values when you need to work with a non-default project | C. 1. Use the default configuration for one project you need to manage. 2. Activate the appropriate configuration when you work with each of your assigned Google Cloud projects. | D. 1. Use the default configuration for one project you need to manage. 2. Use gcloud init to update the configuration values when you need to work with a non-default project.",C
314,"You have deployed multiple Linux instances on Compute Engine. You plan on adding more instances in the coming weeks. You want to be able to access all of these instances through your SSH client over the internet without having to configure specific access on the existing and new instances. You do not want the Compute Engine instances to have a public IP. What should you do?","A. Configure Cloud Identity-Aware Proxy for HTTPS resources. | B. Configure Cloud Identity-Aware Proxy for SSH and TCP resources | C. Create an SSH keypair and store the public key as a project-wide SSH Key. | D. Create an SSH keypair and store the private key as a project-wide SSH Key.",B
315,"You have an object in a Cloud Storage bucket that you want to share with an external company. The object contains sensitive data. You want access to the content to be removed after four hours. The external company does not have a Google account to which you can grant specific user-based access privileges. You want to use the most secure method that requires the fewest steps. What should you do?","A. Create a signed URL with a four-hour expiration and share the URL with the company. | B. Set object access to 'public' and use object lifecycle management to remove the object after four hours. | C. Configure the storage bucket as a static website and furnish the object's URL to the company. | D. Create a new Cloud Storage bucket specifically for the external company to access. Copy the object to that bucket. Delete the bucket after four hours.",D
316,"You manage an App Engine Service that aggregates and visualizes data from BigQuery. The application is deployed with the default App Engine Service account. The data that needs to be visualized resides in a different project managed by another team. You do not have access to this project, but you want your application to be able to read data from the BigQuery dataset. What should you do?","A. Ask the other team to grant your default App Engine Service account the role of BigQuery Job User. | B. Ask the other team to grant your default App Engine Service account the role of BigQuery Data Viewer. | C. In Cloud IAM of your project, ensure that the default App Engine service account has the role of BigQuery Data Viewer. | D. In Cloud IAM of your project, grant a newly created service account from the other team the role of BigQuery Job User in your project.",C
317,"You are deploying a production application on Compute Engine. You want to prevent anyone from accidentally destroying the instance by clicking the wrong button. What should you do?","A. Disable the flag 'Delete boot disk when instance is deleted.' | B. Enable delete protection on the instance. | C. Disable Automatic restart on the instance. | D. Enable Preemptibility on the instance.",B
318,"Your company is moving its entire workload to Compute Engine. Some servers should be accessible through the Internet, and other servers should only be accessible over the internal network. All servers need to be able to talk to each other over specific ports and protocols. The current on-premises network relies on a demilitarized zone (DMZ) for the public servers and a Local Area Network (LAN) for the private servers. You need to design the networking infrastructure on Google Cloud to match these requirements. What should you do?","A. 1. Create a single VPC with a subnet for the DMZ and a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public ingress traffic for the DMZ. | B. 1. Create a single VPC with a subnet for the DMZ and a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public egress traffic for the DMZ. | C. 1. Create a VPC with a subnet for the DMZ and another VPC with a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public ingress traffic for the DMZ. | D. 1. Create a VPC with a subnet for the DMZ and another VPC with a subnet for the LAN. 2. Set up firewall rules to open up relevant traffic between the DMZ and the LAN subnets, and another firewall rule to allow public egress traffic for the DMZ.",C
319,"You have been asked to set up Object Lifecycle Management for objects stored in storage buckets. The objects are written once and accessed frequently for 30 days. After 30 days, the objects are not read again unless there is a special need. The objects should be kept for three years, and you need to minimize cost. What should you do?","A. Set up a policy that uses Nearline storage for 30 days and then moves to Archive storage for three years. | B. Set up a policy that uses Standard storage for 30 days and then moves to Archive storage for three years. | C. Set up a policy that uses Nearline storage for 30 days, then moves the Coldline for one year, and then moves to Archive storage for two years. | D. Set up a policy that uses Standard storage for 30 days, then moves to Coldline for one year, and then moves to Archive storage for two years.",B
320,"You have just created a new project which will be used to deploy a globally distributed application. You will use Cloud Spanner for data storage. You want to create a Cloud Spanner instance. You want to perform the first step in preparation of creating the instance. What should you do?","A. Enable the Cloud Spanner API. | B. Configure your Cloud Spanner instance to be multi-regional. | C. Create a new VPC network with subnetworks in all desired regions. | D. Grant yourself the IAM role of Cloud Spanner Admin.",A
321,"You are deploying an application to App Engine. You want the number of instances to scale based on request rate. You need at least 3 unoccupied instances at all times. Which scaling type should you use?","A. Manual Scaling with 3 instances. | B. Basic Scaling with min_instances set to 3. | C. Basic Scaling with max_instances set to 3. | D. Automatic Scaling with min_idle_instances set to 3.",D
322,"You want to select and configure a solution for storing and archiving data on Google Cloud Platform. You need to support compliance objectives for data from one geographic location. This data is archived after 30 days and needs to be accessed annually. What should you do?","A. Select Multi-Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Coldline Storage. | B. Select Multi-Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Nearline Storage. | C. Select Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Nearline Storage. | D. Select Regional Storage. Add a bucket lifecycle rule that archives data after 30 days to Coldline Storage.",D
323,"You have created an application that is packaged into a Docker image. You want to deploy the Docker image as a workload on Google Kubernetes Engine. What should you do?","A. Upload the image to Cloud Storage and create a Kubernetes Service referencing the image. | B. Upload the image to Cloud Storage and create a Kubernetes Deployment referencing the image. | C. Upload the image to Container Registry and create a Kubernetes Service referencing the image. | D. Upload the image to Container Registry and create a Kubernetes Deployment referencing the image.",D
324,"Your organization needs to grant users access to query datasets in BigQuery but prevent them from accidentally deleting the datasets. You want a solution that follows Google-recommended practices. What should you do?","A. Add users to roles/bigquery user role only, instead of roles/bigquery dataOwner. | B. Add users to roles/bigquery dataEditor role only, instead of roles/bigquery dataOwner. | C. Create a custom role by removing delete permissions, and add users to that role only. | D. Create a custom role by removing delete permissions. Add users to the group, and then add the group to the custom role.",D
325,"You have a workload running on Compute Engine that is critical to your business. You want to ensure that the data on the boot disk of this workload is backed up regularly. You need to be able to restore a backup as quickly as possible in case of disaster. You also want older backups to be cleaned automatically to save on cost. You want to follow Google-recommended practices. What should you do?","A. Create a Cloud Function to create an instance template. | B. Create a snapshot schedule for the disk using the desired interval. | C. Create a cron job to create a new disk from the disk using gcloud. | D. Create a Cloud Task to create an image and export it to Cloud Storage.",B
326,"Your company has workloads running on Compute Engine and on-premises. The Google Cloud Virtual Private Cloud (VPC) is connected to your WAN over a Virtual Private Network (VPN). You need to deploy a new Compute Engine instance and ensure that no public Internet traffic can be routed to it. What should you do?","A. Create the instance without a public IP address. | B. Create the instance with Private Google Access enabled. | C. Create a deny-all egress firewall rule on the VPC network. | D. Create a route on the VPC to route all traffic to the instance over the VPN tunnel.",A
